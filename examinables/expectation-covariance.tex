\section{Expectation and Covariance}

In the introduction, we covered expectation and covariance, which we now revisit from a probabilistic perspective. 

\subsection{Expectation - Ross}

Expectation of a random variable is a probability theory concept, defined as:

\begin{equation}
E[X] = \sum_i x_i P[X=x_i] \label{Expectation}
\end{equation}
Where X is a discrete random variable taking on possible values $x_1,x_2,...,$. The expectation of X is a sum of all the values X can take, weighted by the probability that X assumes the given value. For example, when we roll a fair dice, the expectation is any value would have an equal probability of 1/6, such that the expectation of X, being the random variable taking all possible values of the experiment outcome, would be:
$$
    E[X]=1(\frac{1}{6}) + 2(\frac{1}{6}) + 3(\frac{1}{6}) + 4(\frac{1}{6}) + 5(\frac{1}{6}) + 6(\frac{1}{6})=\frac{7}{2}
$$
It is important to note in this example that the expectation of random variable X is not a value that X could assume, just the expected average value. In code this experiment can be performed likewise:
\begin{verbatim}
a = 0;
b = 6;
% generate 1000 random numbers in the range 1 to 6
r = round((b-a).*rand(1000,1) + a + 0.5);
disp(sum(r)/1000)
    3.4840
\end{verbatim}
Note the multiplication by $\frac{1}{6}$ is implied in the random number generation, where approximately $\frac{1}{6}$ of every number in the range 1 to 6 is expected over 1000 draws.

\subsection{Expectation - Bishop}

Using the notation in Bishop and class slides (week 2), we have, the average value of a function under a \textbf{probability distribution} p(x) is called the \textit{expectation} of f(x) and will be denoted by $\mathbb{E}[f]$. For a discrete distribution, it is given by:
\begin{equation}
    \mathbb{E}[f] = \sum_xp(x)f(x)
\end{equation}
so that the average is weighted by the relative probabilities of the different values of $x$. In the case of continuous variables, expectations are expressed in terms of an integration with respect to the corresponding \textbf{probability density}:
\begin{equation}
    \mathbb{E}[f] = \int p(x)f(x)d(x)
\end{equation}
Given a finite number \textit{N} of points drawn from the probability distribution or probability density, then the expectation can be approximated as:
\begin{equation}
    \mathbb{E}[f] \simeq \frac{1}{N}\sum_{i=1}^Nf(x_n)
\end{equation}
\subsection{Covariance - Bishop}
For two random variables \textit{x} and \textit{y}, the \textit{covariance} expresses the extent to which they vary together, it is a measure of linear dependence defined as:
\begin{align*}
cov[x,y] & = \mathbb{E}_{x,y}\big[\{x-\mathbb{E}[x]\}\{y-\mathbb{E}[y]\}\big] \\
& = \mathbb{E}_{x,y}\big[ xy - x\mathbb{E}[y] - y\mathbb{E}[x] + \mathbb{E}[x]\mathbb{E}[y] \big] \\
& = \mathbb{E}_{x,y}[xy] - \mathbb{E}[x]\mathbb{E}[y] - \mathbb{E}[x]\mathbb{E}[y] + \mathbb{E}[x]\mathbb{E}[y] \\
& = \mathbb{E}_{x,y}[xy] - \mathbb{E}[x]\mathbb{E}[y] \\
\end{align*}
If \textit{x} and \textit{y} are independent the $cov[x,y] = 0$, but the converse is not true e.g. $y=x^2$. Looking at this function in code:
\begin{verbatim}
>> cov([-3 -2 -1 0 1 2 3], [9 4 1 0 1 4 9])
ans =
    4.6667         0
         0   14.0000    
\end{verbatim}
we see that the covariance, as defined in \ref{covariance}, between \textit{x} and \textit{y} is 0, though (specially in a graph) we can see they are clearly varying together. In case of two vectors of random variables \textbf{x} and \textbf{y}, the covariance is a matrix:
\begin{align*}
cov[\boldsymbol{\mathrm{x}},\boldsymbol{\mathrm{y}}] & = \mathbb{E}_{\boldsymbol{\mathrm{x}},\boldsymbol{\mathrm{y}}}\big[\{\boldsymbol{\mathrm{x}}-\mathbb{E}[\boldsymbol{\mathrm{x}}]\}\{\boldsymbol{\mathrm{y}}^\mathrm{T}-\mathbb{E}[\boldsymbol{\mathrm{y}}^\mathrm{T}]\}\big] \\
& = \mathbb{E}_{\boldsymbol{\mathrm{x}},\boldsymbol{\mathrm{y}}}\big[ \boldsymbol{\mathrm{x}}\boldsymbol{\mathrm{y}}^\mathrm{T} \big] - \mathbb{E}[\boldsymbol{\mathrm{x}}] \mathbb{E}[\boldsymbol{\mathrm{y}}^\mathrm{T}]  
\end{align*}



